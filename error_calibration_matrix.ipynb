{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Error Calibration Matrix Generation\n",
    "\n",
    "This notebook creates a 16Ã—16 error calibration matrix by comparing ideal and noisy quantum results using 100,000 test records.\n",
    "The calibration matrix will help correct systematic errors in the quantum circuit predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from scipy.linalg import inv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## Load Test Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "data-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: X_test: (115920, 168, 9), y_test: (115920, 72)\n",
      "Location data shape: (115920,)\n",
      "Using 100000 samples for calibration matrix\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "X_test = np.load('Testing/X_test.npy')\n",
    "y_test = np.load('Testing/y_test.npy')\n",
    "loc_test = np.load('Testing/loc_test.npy')\n",
    "\n",
    "print(f\"Test data shape: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print(f\"Location data shape: {loc_test.shape}\")\n",
    "\n",
    "# Limit to 100,000 records for calibration as requested\n",
    "n_calibration_samples = min(100000, len(X_test))\n",
    "print(f\"Using {n_calibration_samples} samples for calibration matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-setup",
   "metadata": {},
   "source": [
    "## Model Architecture Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "model-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Setup ideal device\n",
    "dev_ideal = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
    "\n",
    "# Setup noisy device (if available) - adjust based on your setup\n",
    "\n",
    "from qiskit_aer.noise import NoiseModel, depolarizing_error\n",
    "\n",
    "# Create a simple noise model\n",
    "noise_model = NoiseModel()\n",
    "error = depolarizing_error(0.01, 1)  # 1% error rate for single-qubit gates\n",
    "noise_model.add_all_qubit_quantum_error(error, ['rx', 'ry', 'rz'])\n",
    "\n",
    "dev_noisy = qml.device(\"qiskit.aer\", wires=n_qubits, \n",
    "                backend=\"aer_simulator\", \n",
    "                noise_model=noise_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "quantum-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev_ideal, interface=\"torch\")\n",
    "def q_circuit_ideal(inputs, weights):\n",
    "    \"\"\"Ideal quantum circuit without noise.\"\"\"\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "\n",
    "@qml.qnode(dev_noisy, interface=\"torch\")\n",
    "def q_circuit_noisy(inputs, weights):\n",
    "    \"\"\"Noisy quantum circuit.\"\"\"\n",
    "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-class",
   "metadata": {},
   "source": [
    "## Model Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "model-class-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantum-Classical LSTM model for multi-step forecasting.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_lstm_units=32, n_qubits=4, num_layers=1, n_layers=3, output_len=72, device_type='ideal'):\n",
    "        super(QLSTMModel, self).__init__()\n",
    "        \n",
    "        self.device_type = device_type\n",
    "        \n",
    "        # Classical LSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_lstm_units,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Classical layer to map LSTM output to quantum input\n",
    "        self.classical_to_quantum = nn.Linear(n_lstm_units, n_qubits)\n",
    "        \n",
    "        # Quantum Layer - select based on device type\n",
    "        if device_type == 'ideal':\n",
    "            weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
    "            self.q_layer = qml.qnn.TorchLayer(q_circuit_ideal, weight_shapes)\n",
    "        else:\n",
    "            weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
    "            self.q_layer = qml.qnn.TorchLayer(q_circuit_noisy, weight_shapes)\n",
    "        \n",
    "        # Classical layer to map quantum output to predictions\n",
    "        self.quantum_to_output = nn.Linear(n_qubits, output_len)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Extract features from last timestep\n",
    "        final_lstm_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Map to quantum input\n",
    "        quantum_input = self.classical_to_quantum(final_lstm_output)\n",
    "        \n",
    "        # Pass through quantum circuit\n",
    "        quantum_features = self.q_layer(quantum_input)\n",
    "        \n",
    "        # Map to output\n",
    "        output = self.quantum_to_output(quantum_features)\n",
    "        \n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generate-predictions",
   "metadata": {},
   "source": [
    "## Generate Ideal and Noisy Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction-generation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n",
      "\n",
      "Model Architecture:\n",
      "QLSTMModel(\n",
      "  (lstm): LSTM(9, 32, batch_first=True)\n",
      "  (classical_to_quantum): Linear(in_features=32, out_features=4, bias=True)\n",
      "  (q_layer): <Quantum Torch Layer: func=q_circuit_ideal>\n",
      "  (quantum_to_output): Linear(in_features=4, out_features=72, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create models\n",
    "model_ideal = QLSTMModel(\n",
    "    n_features=9,\n",
    "    n_lstm_units=32,\n",
    "    n_qubits=n_qubits,\n",
    "    num_layers=1,\n",
    "    n_layers=n_layers,\n",
    "    output_len=72,\n",
    "    device_type='ideal'\n",
    ").to(device)\n",
    "\n",
    "model_noisy = QLSTMModel(\n",
    "    n_features=9,\n",
    "    n_lstm_units=32,\n",
    "    n_qubits=n_qubits,\n",
    "    num_layers=1,\n",
    "    n_layers=n_layers,\n",
    "    output_len=72,\n",
    "    device_type='noisy'\n",
    ").to(device)\n",
    "\n",
    "model_ideal.load_state_dict(torch.load('Testing//best_qlstm_model_multistep.pth', weights_only=True))\n",
    "model_noisy.load_state_dict(torch.load('Testing//best_qlstm_model_multistep.pth', weights_only=True))\n",
    "print(\"Models loaded successfully!\")\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(model_ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "batch-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantum_state_probabilities(model, dataloader, n_samples):\n",
    "    \"\"\"Extract quantum state probabilities from model predictions.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    sample_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in dataloader:\n",
    "            if sample_count >= n_samples:\n",
    "                break\n",
    "                \n",
    "            X_batch = X_batch.to(device)\n",
    "            \n",
    "            # Get quantum features (before final linear layer)\n",
    "            lstm_out, _ = model.lstm(X_batch)\n",
    "            final_lstm_output = lstm_out[:, -1, :]\n",
    "            quantum_input = model.classical_to_quantum(final_lstm_output)\n",
    "            quantum_features = model.q_layer(quantum_input)\n",
    "            \n",
    "            # Convert to probability distributions over 16 states (2^4)\n",
    "            # Map 4 qubit expectation values to 16-dimensional probability vector\n",
    "            batch_probs = convert_to_state_probs(quantum_features)\n",
    "            \n",
    "            all_predictions.extend(batch_probs.cpu().numpy())\n",
    "            sample_count += len(X_batch)\n",
    "            \n",
    "            if sample_count % 10000 == 0:\n",
    "                print(f\"Processed {sample_count} samples...\")\n",
    "    \n",
    "    return np.array(all_predictions[:n_samples])\n",
    "\n",
    "def convert_to_state_probs(quantum_features):\n",
    "    \"\"\"Convert 4 qubit expectation values to 16-state probability distribution.\"\"\"\n",
    "    batch_size = quantum_features.shape[0]\n",
    "    \n",
    "    # Create 16-dimensional state probabilities from 4 qubit measurements\n",
    "    # This is a simplified conversion - you may need to adjust based on your specific measurement scheme\n",
    "    state_probs = torch.zeros(batch_size, 16).to(quantum_features.device)\n",
    "    \n",
    "    # Convert expectation values to probabilities\n",
    "    # For each qubit, <Z> = p0 - p1, so p0 = (1 + <Z>)/2, p1 = (1 - <Z>)/2\n",
    "    qubit_probs = (1 + quantum_features) / 2  # Convert from [-1,1] to [0,1]\n",
    "    \n",
    "    # Generate all 16 computational basis states\n",
    "    for i in range(16):\n",
    "        # Convert index to 4-bit binary representation\n",
    "        bits = [(i >> j) & 1 for j in range(4)]\n",
    "        \n",
    "        # Calculate probability for this computational basis state\n",
    "        prob = torch.ones(batch_size).to(quantum_features.device)\n",
    "        for qubit_idx, bit in enumerate(bits):\n",
    "            if bit == 0:\n",
    "                prob *= qubit_probs[:, qubit_idx]\n",
    "            else:\n",
    "                prob *= (1 - qubit_probs[:, qubit_idx])\n",
    "        \n",
    "        state_probs[:, i] = prob\n",
    "    \n",
    "    # Normalize to ensure probabilities sum to 1\n",
    "    state_probs = state_probs / (state_probs.sum(dim=1, keepdim=True) + 1e-8)\n",
    "    \n",
    "    return state_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ideal quantum state probabilities...\n",
      "Processed 100000 samples...\n",
      "Generating noisy quantum state probabilities...\n"
     ]
    }
   ],
   "source": [
    "# Create data loader\n",
    "batch_size = 256\n",
    "calibration_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_test[:n_calibration_samples]), \n",
    "    torch.from_numpy(y_test[:n_calibration_samples])\n",
    ")\n",
    "calibration_loader = DataLoader(calibration_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Generating ideal quantum state probabilities...\")\n",
    "ideal_probs = get_quantum_state_probabilities(model_ideal, calibration_loader, n_calibration_samples)\n",
    "\n",
    "print(\"Generating noisy quantum state probabilities...\")\n",
    "noisy_probs = get_quantum_state_probabilities(model_noisy, calibration_loader, n_calibration_samples)\n",
    "\n",
    "print(f\"\\nGenerated probability distributions:\")\n",
    "print(f\"Ideal probabilities shape: {ideal_probs.shape}\")\n",
    "print(f\"Noisy probabilities shape: {noisy_probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "calibration-matrix",
   "metadata": {},
   "source": [
    "## Generate 16Ã—16 Calibration Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calibration-computation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_calibration_matrix(ideal_probs, noisy_probs):\n",
    "    \"\"\"\n",
    "    Create a 16x16 calibration matrix from ideal and noisy probability distributions.\n",
    "    \n",
    "    The calibration matrix M satisfies: P_ideal = M @ P_noisy\n",
    "    where P_ideal and P_noisy are the probability distributions.\n",
    "    \n",
    "    Returns:\n",
    "        calibration_matrix: 16x16 matrix to correct noisy measurements\n",
    "    \"\"\"\n",
    "    print(\"Computing calibration matrix...\")\n",
    "    \n",
    "    # Method 1: Direct least squares solution\n",
    "    # We want to find M such that ideal_probs â‰ˆ M @ noisy_probs.T\n",
    "    # This gives us: M = ideal_probs @ noisy_probs^+\n",
    "    # where ^+ denotes the Moore-Penrose pseudoinverse\n",
    "    \n",
    "    try:\n",
    "        # Compute pseudoinverse of noisy probabilities\n",
    "        noisy_pinv = np.linalg.pinv(noisy_probs.T)\n",
    "        calibration_matrix = ideal_probs.T @ noisy_pinv.T\n",
    "        \n",
    "        print(f\"Calibration matrix computed using pseudoinverse method\")\n",
    "        \n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Pseudoinverse method failed, using regularized approach...\")\n",
    "        \n",
    "        # Method 2: Regularized least squares\n",
    "        lambda_reg = 1e-6\n",
    "        A = noisy_probs.T @ noisy_probs + lambda_reg * np.eye(16)\n",
    "        b = noisy_probs.T @ ideal_probs\n",
    "        calibration_matrix = np.linalg.solve(A, b).T\n",
    "    \n",
    "    # Ensure the matrix is properly normalized (each column should sum to 1)\n",
    "    # This ensures that applying the calibration preserves probability normalization\n",
    "    col_sums = np.sum(calibration_matrix, axis=0)\n",
    "    col_sums[col_sums == 0] = 1  # Avoid division by zero\n",
    "    calibration_matrix = calibration_matrix / col_sums[np.newaxis, :]\n",
    "    \n",
    "    return calibration_matrix\n",
    "\n",
    "# Generate calibration matrix\n",
    "calibration_matrix = create_calibration_matrix(ideal_probs, noisy_probs)\n",
    "\n",
    "print(f\"\\nCalibration matrix shape: {calibration_matrix.shape}\")\n",
    "print(f\"Matrix properties:\")\n",
    "print(f\"  - Min value: {np.min(calibration_matrix):.6f}\")\n",
    "print(f\"  - Max value: {np.max(calibration_matrix):.6f}\")\n",
    "print(f\"  - Column sums range: [{np.min(np.sum(calibration_matrix, axis=0)):.6f}, {np.max(np.sum(calibration_matrix, axis=0)):.6f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-matrix",
   "metadata": {},
   "source": [
    "## Visualize Calibration Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the calibration matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Heatmap of calibration matrix\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(calibration_matrix, annot=False, cmap='RdBu_r', center=0, \n",
    "            square=True, cbar_kws={'label': 'Calibration Weight'})\n",
    "plt.title('16Ã—16 Error Calibration Matrix')\n",
    "plt.xlabel('Noisy State Index')\n",
    "plt.ylabel('Ideal State Index')\n",
    "\n",
    "# Plot 2: Diagonal elements (should be close to 1 for good calibration)\n",
    "plt.subplot(2, 2, 2)\n",
    "diagonal_elements = np.diag(calibration_matrix)\n",
    "plt.plot(diagonal_elements, 'bo-', markersize=4)\n",
    "plt.axhline(y=1, color='r', linestyle='--', alpha=0.7, label='Ideal value')\n",
    "plt.title('Diagonal Elements of Calibration Matrix')\n",
    "plt.xlabel('State Index')\n",
    "plt.ylabel('Calibration Weight')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Off-diagonal elements distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "off_diagonal = calibration_matrix[~np.eye(16, dtype=bool)]\n",
    "plt.hist(off_diagonal, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribution of Off-Diagonal Elements')\n",
    "plt.xlabel('Calibration Weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Column sums (should be close to 1)\n",
    "plt.subplot(2, 2, 4)\n",
    "column_sums = np.sum(calibration_matrix, axis=0)\n",
    "plt.plot(column_sums, 'go-', markersize=4)\n",
    "plt.axhline(y=1, color='r', linestyle='--', alpha=0.7, label='Ideal value')\n",
    "plt.title('Column Sums (Probability Conservation)')\n",
    "plt.xlabel('State Index')\n",
    "plt.ylabel('Sum')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('calibration_matrix_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation",
   "metadata": {},
   "source": [
    "## Validate Calibration Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_calibration(calibration_matrix, ideal_probs, noisy_probs, n_test=1000):\n",
    "    \"\"\"\n",
    "    Validate the calibration matrix by testing on a subset of data.\n",
    "    \"\"\"\n",
    "    print(\"Validating calibration matrix...\")\n",
    "    \n",
    "    # Take a subset for validation\n",
    "    test_ideal = ideal_probs[:n_test]\n",
    "    test_noisy = noisy_probs[:n_test]\n",
    "    \n",
    "    # Apply calibration matrix to noisy measurements\n",
    "    calibrated_probs = (calibration_matrix @ test_noisy.T).T\n",
    "    \n",
    "    # Compute metrics\n",
    "    # 1. Mean squared error between ideal and calibrated\n",
    "    mse_before = np.mean((test_ideal - test_noisy) ** 2)\n",
    "    mse_after = np.mean((test_ideal - calibrated_probs) ** 2)\n",
    "    \n",
    "    # 2. Fidelity (quantum state overlap)\n",
    "    fidelity_before = np.mean([np.sqrt(np.sum(np.sqrt(p1 * p2))) \n",
    "                              for p1, p2 in zip(test_ideal, test_noisy)])\n",
    "    fidelity_after = np.mean([np.sqrt(np.sum(np.sqrt(p1 * p2))) \n",
    "                             for p1, p2 in zip(test_ideal, calibrated_probs)])\n",
    "    \n",
    "    # 3. Total variation distance\n",
    "    tv_before = np.mean([0.5 * np.sum(np.abs(p1 - p2)) \n",
    "                        for p1, p2 in zip(test_ideal, test_noisy)])\n",
    "    tv_after = np.mean([0.5 * np.sum(np.abs(p1 - p2)) \n",
    "                       for p1, p2 in zip(test_ideal, calibrated_probs)])\n",
    "    \n",
    "    print(f\"\\nValidation Results (on {n_test} samples):\")\n",
    "    print(f\"Mean Squared Error:\")\n",
    "    print(f\"  Before calibration: {mse_before:.6f}\")\n",
    "    print(f\"  After calibration:  {mse_after:.6f}\")\n",
    "    print(f\"  Improvement: {((mse_before - mse_after) / mse_before * 100):.2f}%\")\n",
    "    \n",
    "    print(f\"\\nFidelity (higher is better):\")\n",
    "    print(f\"  Before calibration: {fidelity_before:.6f}\")\n",
    "    print(f\"  After calibration:  {fidelity_after:.6f}\")\n",
    "    print(f\"  Improvement: {((fidelity_after - fidelity_before) / fidelity_before * 100):.2f}%\")\n",
    "    \n",
    "    print(f\"\\nTotal Variation Distance (lower is better):\")\n",
    "    print(f\"  Before calibration: {tv_before:.6f}\")\n",
    "    print(f\"  After calibration:  {tv_after:.6f}\")\n",
    "    print(f\"  Improvement: {((tv_before - tv_after) / tv_before * 100):.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'mse_before': mse_before,\n",
    "        'mse_after': mse_after,\n",
    "        'fidelity_before': fidelity_before,\n",
    "        'fidelity_after': fidelity_after,\n",
    "        'tv_before': tv_before,\n",
    "        'tv_after': tv_after\n",
    "    }\n",
    "\n",
    "# Validate the calibration matrix\n",
    "validation_results = validate_calibration(calibration_matrix, ideal_probs, noisy_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-results",
   "metadata": {},
   "source": [
    "## Save Calibration Matrix and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save calibration matrix in multiple formats\n",
    "\n",
    "# 1. Save as NumPy array\n",
    "np.save('calibration_matrix_16x16.npy', calibration_matrix)\n",
    "\n",
    "# 2. Save as CSV for easy viewing\n",
    "np.savetxt('calibration_matrix_16x16.csv', calibration_matrix, delimiter=',')\n",
    "\n",
    "# 3. Save detailed results as pickle\n",
    "calibration_data = {\n",
    "    'calibration_matrix': calibration_matrix,\n",
    "    'validation_results': validation_results,\n",
    "    'n_calibration_samples': n_calibration_samples,\n",
    "    'ideal_probs_sample': ideal_probs[:100],  # Save first 100 for reference\n",
    "    'noisy_probs_sample': noisy_probs[:100],\n",
    "    'matrix_stats': {\n",
    "        'diagonal_elements': np.diag(calibration_matrix),\n",
    "        'column_sums': np.sum(calibration_matrix, axis=0),\n",
    "        'row_sums': np.sum(calibration_matrix, axis=1),\n",
    "        'condition_number': np.linalg.cond(calibration_matrix)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('calibration_data.pkl', 'wb') as f:\n",
    "    pickle.dump(calibration_data, f)\n",
    "\n",
    "# 4. Save summary report\n",
    "with open('calibration_report.txt', 'w') as f:\n",
    "    f.write(\"QUANTUM ERROR CALIBRATION MATRIX REPORT\\n\")\n",
    "    f.write(\"=\" * 45 + \"\\n\\n\")\n",
    "    f.write(f\"Generated: {pd.Timestamp.now()}\\n\")\n",
    "    f.write(f\"Calibration samples used: {n_calibration_samples:,}\\n\")\n",
    "    f.write(f\"Matrix size: 16 Ã— 16\\n\\n\")\n",
    "    \n",
    "    f.write(\"MATRIX PROPERTIES:\\n\")\n",
    "    f.write(f\"  Condition number: {np.linalg.cond(calibration_matrix):.2e}\\n\")\n",
    "    f.write(f\"  Min element: {np.min(calibration_matrix):.6f}\\n\")\n",
    "    f.write(f\"  Max element: {np.max(calibration_matrix):.6f}\\n\")\n",
    "    f.write(f\"  Mean diagonal: {np.mean(np.diag(calibration_matrix)):.6f}\\n\")\n",
    "    f.write(f\"  Std diagonal: {np.std(np.diag(calibration_matrix)):.6f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"VALIDATION RESULTS:\\n\")\n",
    "    for key, value in validation_results.items():\n",
    "        f.write(f\"  {key}: {value:.6f}\\n\")\n",
    "    \n",
    "    f.write(\"\\nUSAGE:\\n\")\n",
    "    f.write(\"To apply calibration to noisy measurements P_noisy:\\n\")\n",
    "    f.write(\"  P_calibrated = calibration_matrix @ P_noisy\\n\")\n",
    "    f.write(\"\\nFile outputs:\\n\")\n",
    "    f.write(\"  - calibration_matrix_16x16.npy: NumPy binary format\\n\")\n",
    "    f.write(\"  - calibration_matrix_16x16.csv: CSV format\\n\")\n",
    "    f.write(\"  - calibration_data.pkl: Complete dataset\\n\")\n",
    "    f.write(\"  - calibration_matrix_analysis.png: Visualization\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CALIBRATION MATRIX GENERATION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Matrix saved as: calibration_matrix_16x16.npy\")\n",
    "print(f\"CSV format: calibration_matrix_16x16.csv\")\n",
    "print(f\"Complete data: calibration_data.pkl\")\n",
    "print(f\"Report: calibration_report.txt\")\n",
    "print(f\"Visualization: calibration_matrix_analysis.png\")\n",
    "print(\"\\nMatrix ready for error correction in quantum predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-example",
   "metadata": {},
   "source": [
    "## Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usage-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how to use the calibration matrix\n",
    "\n",
    "def apply_error_correction(noisy_measurements, calnibratio_matrix):\n",
    "    \"\"\"\n",
    "    Apply error correction to noisy quantum measurements.\n",
    "    \n",
    "    Args:\n",
    "        noisy_measurements: Array of shape (n_samples, 16) containing noisy probabilities\n",
    "        calibration_matrix: 16x16 calibration matrix\n",
    "    \n",
    "    Returns:\n",
    "        corrected_measurements: Error-corrected probabilities\n",
    "    \"\"\"\n",
    "    corrected = (calibration_matrix @ noisy_measurements.T).T\n",
    "    \n",
    "    # Ensure probabilities are non-negative and normalized\n",
    "    corrected = np.maximum(corrected, 0)\n",
    "    corrected = corrected / np.sum(corrected, axis=1, keepdim=True)\n",
    "    \n",
    "    return corrected\n",
    "\n",
    "# Example usage with first 10 samples\n",
    "print(\"EXAMPLE: Applying error correction to sample measurements\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "sample_noisy = noisy_probs[:10]\n",
    "sample_ideal = ideal_probs[:10]\n",
    "sample_corrected = apply_error_correction(sample_noisy, calibration_matrix)\n",
    "\n",
    "print(f\"Sample 1 comparison:\")\n",
    "print(f\"  Ideal:     {sample_ideal[0][:8]:.4f} ... (first 8 states)\")\n",
    "print(f\"  Noisy:     {sample_noisy[0][:8]:.4f} ... (first 8 states)\")\n",
    "print(f\"  Corrected: {sample_corrected[0][:8]:.4f} ... (first 8 states)\")\n",
    "\n",
    "# Calculate improvement\n",
    "error_before = np.mean((sample_ideal - sample_noisy) ** 2)\n",
    "error_after = np.mean((sample_ideal - sample_corrected) ** 2)\n",
    "improvement = (error_before - error_after) / error_before * 100\n",
    "\n",
    "print(f\"\\nMSE improvement on sample: {improvement:.2f}%\")\n",
    "print(\"\\nCalibration matrix ready for integration into your quantum pipeline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and next steps\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"            CALIBRATION MATRIX SUMMARY            \")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ“ Generated 16Ã—16 error calibration matrix\")\n",
    "print(f\"âœ“ Used {n_calibration_samples:,} quantum measurement pairs\")\n",
    "print(f\"âœ“ Matrix condition number: {np.linalg.cond(calibration_matrix):.2e}\")\n",
    "print(f\"âœ“ Validation MSE improvement: {((validation_results['mse_before'] - validation_results['mse_after']) / validation_results['mse_before'] * 100):.2f}%\")\n",
    "print(f\"âœ“ Files saved and ready for use\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"1. Integrate calibration_matrix_16x16.npy into your quantum pipeline\")\n",
    "print(\"2. Apply correction: P_corrected = calibration_matrix @ P_noisy\")\n",
    "print(\"3. Monitor performance improvements in downstream tasks\")\n",
    "print(\"4. Consider retraining calibration matrix periodically\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
