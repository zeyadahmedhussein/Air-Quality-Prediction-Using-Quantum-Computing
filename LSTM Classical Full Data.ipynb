{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e2053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2054fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"C:\\Users\\ziyad\\Downloads\\final_air_quality_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3174d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_min, lat_max = df['lat'].min(), df['lat'].max()\n",
    "lon_min, lon_max = df['lon'].min(), df['lon'].max()\n",
    "grid_size = 20\n",
    "lat_bins = np.linspace(lat_min, lat_max, grid_size + 1)\n",
    "lon_bins = np.linspace(lon_min, lon_max, grid_size + 1)\n",
    "\n",
    "lat_positions = pd.cut(df['lat'], bins=lat_bins, labels=False, include_lowest=True)\n",
    "lon_positions = pd.cut(df['lon'], bins=lon_bins, labels=False, include_lowest=True)\n",
    "\n",
    "df['location'] = lat_positions * grid_size + lon_positions\n",
    "\n",
    "df['location'] = df['location'].fillna(0).astype(int)\n",
    "df['class'] = df['class'].isin(['Good', 'Moderate']).astype(int)\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df.drop(columns=['lat', 'lon', 'source_file', 'time'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32684d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(df, year):\n",
    "    df = df[(df['year'] == year)]\n",
    "    print(f\"Dataset shape: {df.shape}\")    \n",
    "    le_location = LabelEncoder()\n",
    "    df['location_encoded'] = le_location.fit_transform(df['location'])\n",
    "    df = df.sort_values(['location_encoded', 'month', 'day', 'hour'])\n",
    "    feature_columns = [col for col in df.columns if col not in [\n",
    "        'class', 'location', 'month', 'day', 'hour', 'location_encoded'\n",
    "    ]]\n",
    "    feature_columns.append('location_encoded')\n",
    "    \n",
    "    print(f\"Feature columns: {feature_columns}\")\n",
    "    print(f\"Number of features: {len(feature_columns)}\")\n",
    "    \n",
    "    return df, feature_columns, le_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a4aa64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history for multi-step forecasting\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (per timestep)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('classical_training_history_multistep.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_example_predictions(model, X_test, y_test, num_examples=3):\n",
    "    \"\"\"Plot example predictions vs actual values\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sample_indices = np.random.choice(len(X_test), num_examples, replace=False)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5 * num_examples))\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            X_sample = torch.tensor(X_test[idx:idx+1]).to(device)\n",
    "            y_true = y_test[idx]\n",
    "            y_pred = model(X_sample).cpu().numpy()[0]\n",
    "            \n",
    "            plt.subplot(num_examples, 1, i+1)\n",
    "            hours = np.arange(1, OUTPUT_SEQUENCE_LENGTH + 1)\n",
    "            plt.plot(hours, y_true, 'bo-', label='Actual')\n",
    "            plt.plot(hours, y_pred, 'ro--', label='Predicted')\n",
    "            plt.title(f'Example {i+1}: Air Quality Prediction (Next {OUTPUT_SEQUENCE_LENGTH} Hours)')\n",
    "            plt.xlabel('Hours Ahead')\n",
    "            plt.ylabel('Air Quality Class')\n",
    "            plt.yticks([0, 1], ['Good', 'Poor'])\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('example_predictions.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af08978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SEQUENCE_LENGTH = 168\n",
    "OUTPUT_SEQUENCE_LENGTH = 72 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c17559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Builds a purely classical LSTM model for multi-step time-series forecasting.\n",
    "    Predicts the next 3 days (72 hours) of air quality classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_lstm_units=4, output_len=OUTPUT_SEQUENCE_LENGTH):\n",
    "        super(ClassicalLSTMModel, self).__init__()\n",
    "        \n",
    "        # 1. Standard LSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_lstm_units,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # 2. Feed-Forward Classifier for multi-step prediction\n",
    "        self.classifier = nn.Linear(n_lstm_units, output_len)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        # 1. Pass data through the classical LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # 2. Extract features from the last timestep\n",
    "        final_lstm_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # 3. Pass through classifier to get predictions for all timesteps\n",
    "        output = self.classifier(final_lstm_output)\n",
    "        \n",
    "        # 4. Apply sigmoid activation to get probabilities for each timestep\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b32fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_pytorch(model, train_loader, val_loader, optimizer, epochs, initial_epoch=0, patience=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'val_loss': [],\n",
    "        'accuracy': [],\n",
    "        'val_accuracy': []\n",
    "    }\n",
    "\n",
    "    criterion = torch.nn.BCELoss() \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(initial_epoch, epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.numel()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch.float())\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "                preds = (outputs > 0.5).float()\n",
    "                val_correct += (preds == y_batch).sum().item()\n",
    "                val_total += y_batch.numel()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        # Append to history\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Acc: {train_acc:.4f} Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return model, history, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bcc4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_memory_efficient(df, feature_columns, \n",
    "                                    input_len=INPUT_SEQUENCE_LENGTH,\n",
    "                                    output_len=OUTPUT_SEQUENCE_LENGTH,\n",
    "                                    stride=24):\n",
    "    \"\"\"\n",
    "    Create sequences for multi-step forecasting.\n",
    "    Input: `input_len` hours of data\n",
    "    Output: `output_len` hours of predictions\n",
    "    \"\"\"\n",
    "    print(f\"Creating sequences with input length={input_len}, output length={output_len}...\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df[feature_columns])\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=feature_columns, index=df.index)\n",
    "    \n",
    "    X_sequences, y_sequences, location_indices = [], [], []\n",
    "    unique_locations = df['location_encoded'].unique()\n",
    "    \n",
    "    for i, loc in enumerate(unique_locations):\n",
    "        loc_df = df[df['location_encoded'] == loc]\n",
    "        loc_X = X_scaled.loc[loc_df.index]\n",
    "        loc_y = loc_df['class'].values\n",
    "        \n",
    "        # Calculate valid range to ensure we have enough data for output sequence\n",
    "        max_start_idx = len(loc_df) - input_len - output_len\n",
    "        \n",
    "        for j in range(0, max_start_idx, stride):\n",
    "            X_seq = loc_X.iloc[j : j + input_len].values\n",
    "            # Get the next `output_len` hours of class data\n",
    "            y_target = loc_y[j + input_len : j + input_len + output_len]\n",
    "            \n",
    "            X_sequences.append(X_seq)\n",
    "            y_sequences.append(y_target)\n",
    "            location_indices.append(loc)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed location {i+1}/{len(unique_locations)}\")\n",
    "    \n",
    "    X_sequences = np.array(X_sequences, dtype=np.float32)\n",
    "    y_sequences = np.array(y_sequences, dtype=np.float32)\n",
    "    location_indices = np.array(location_indices)\n",
    "    \n",
    "    print(f\"Total sequences: {X_sequences.shape[0]}\")\n",
    "    print(f\"Input sequence shape: {X_sequences.shape}\")\n",
    "    print(f\"Output sequence shape: {y_sequences.shape}\")\n",
    "    \n",
    "    return X_sequences, y_sequences, location_indices, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a197b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing year: 2021\n",
      "Dataset shape: (2035200, 58)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziyad\\AppData\\Local\\Temp\\ipykernel_7572\\839366471.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['location_encoded'] = le_location.fit_transform(df['location'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: ['DUEXTTAU', 'BCFLUXU', 'OCFLUXV', 'BCANGSTR', 'SUFLUXV', 'SSSMASS25', 'SSSMASS', 'OCSMASS', 'BCCMASS', 'BCSMASS', 'SO4CMASS', 'SSFLUXU', 'DUCMASS', 'SSEXTTAU', 'SO2CMASS', 'DUSCATAU', 'OCANGSTR', 'OCCMASS', 'TOTEXTTAU', 'DUSCAT25', 'TOTANGSTR', 'DMSCMASS', 'SSEXTT25', 'DUANGSTR', 'DMSSMASS', 'BCEXTTAU', 'SSSCATAU', 'DUFLUXV', 'DUFLUXU', 'SUEXTTAU', 'SSFLUXV', 'BCSCATAU', 'DUCMASS25', 'OCEXTTAU', 'SUANGSTR', 'SSSCAT25', 'SSCMASS25', 'SO4SMASS', 'DUSMASS', 'SUFLUXU', 'BCFLUXV', 'DUSMASS25', 'SSCMASS', 'SUSCATAU', 'SO2SMASS', 'SSANGSTR', 'DUEXTT25', 'OCFLUXU', 'OCSCATAU', 'TOTSCATAU', 'PM25_MERRA2', 'PM25_ug_m3', 'year', 'location_encoded']\n",
      "Number of features: 54\n",
      "Creating sequences with input length=168, output length=72...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ziyad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed location 100/400\n",
      "Processed location 200/400\n",
      "Processed location 300/400\n",
      "Processed location 400/400\n",
      "Total sequences: 80800\n",
      "Input sequence shape: (80800, 168, 54)\n",
      "Output sequence shape: (80800, 72)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Train for this year\u001b[39;00m\n\u001b[32m     47\u001b[39m epochs_this_year = \u001b[32m20\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m model, history, optimizer = \u001b[43mtrain_model_pytorch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_epochs_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs_this_year\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_epochs_done\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m     53\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m plot_training_history(history)\n\u001b[32m     55\u001b[39m total_epochs_done += epochs_this_year\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtrain_model_pytorch\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, epochs, initial_epoch, patience)\u001b[39m\n\u001b[32m     24\u001b[39m outputs = model(X_batch)\n\u001b[32m     25\u001b[39m loss = criterion(outputs, y_batch.float())\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m optimizer.step()\n\u001b[32m     29\u001b[39m running_loss += loss.item() * X_batch.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ziyad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ziyad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ziyad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "for idx, year in enumerate(years):\n",
    "    print(f\"\\nProcessing year: {year}\")\n",
    "    \n",
    "    df_year, feature_columns, le_location = load_and_prepare_data(df, year)\n",
    "    \n",
    "    if model is None:  # First year, build the model\n",
    "        n_lstm_units = 8\n",
    "        model = ClassicalLSTMModel(\n",
    "            n_features=len(feature_columns), \n",
    "            n_lstm_units=n_lstm_units,  \n",
    "            output_len=OUTPUT_SEQUENCE_LENGTH\n",
    "        )\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    unique_locs_subset = df_year['location_encoded'].unique()[:400]\n",
    "    df_subset = df_year[df_year['location_encoded'].isin(unique_locs_subset)]\n",
    "    \n",
    "    # Create sequences\n",
    "    X_sequences, y_sequences, location_indices, scaler = create_sequences_memory_efficient(\n",
    "        df_subset, \n",
    "        feature_columns, \n",
    "        input_len=INPUT_SEQUENCE_LENGTH,\n",
    "        output_len=OUTPUT_SEQUENCE_LENGTH,\n",
    "        stride=24\n",
    "    )\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_sequences, y_sequences, test_size=0.2, random_state=42, stratify=location_indices\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Free memory\n",
    "    del X_sequences, y_sequences, location_indices, X_train, X_test, y_train, y_test\n",
    "    gc.collect()\n",
    "    \n",
    "    # Train for this year\n",
    "    epochs_this_year = 20\n",
    "    model, history, optimizer = train_model_pytorch(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        epochs=total_epochs_done + epochs_this_year,\n",
    "        initial_epoch=total_epochs_done,\n",
    "        patience=5\n",
    "    )\n",
    "    plot_training_history(history)\n",
    "    total_epochs_done += epochs_this_year\n",
    "    \n",
    "    # Merge history\n",
    "    for key in full_history.keys():\n",
    "        full_history[key].extend(history[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(full_history)\n",
    "\n",
    "\n",
    "X_test_plot = []\n",
    "y_test_plot = []\n",
    "for X_batch, y_batch in val_loader:\n",
    "    X_test_plot.append(X_batch)\n",
    "    y_test_plot.append(y_batch)\n",
    "X_test_plot = torch.cat(X_test_plot, dim=0).numpy()\n",
    "y_test_plot = torch.cat(y_test_plot, dim=0).numpy()\n",
    "\n",
    "plot_example_predictions(model, X_test_plot, y_test_plot, num_examples=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
