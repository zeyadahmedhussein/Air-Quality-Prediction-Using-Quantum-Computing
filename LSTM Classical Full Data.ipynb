{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e2053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2054fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"C:\\Users\\ziyad\\Downloads\\final_air_quality_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3174d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_min, lat_max = df['lat'].min(), df['lat'].max()\n",
    "lon_min, lon_max = df['lon'].min(), df['lon'].max()\n",
    "grid_size = 20\n",
    "lat_bins = np.linspace(lat_min, lat_max, grid_size + 1)\n",
    "lon_bins = np.linspace(lon_min, lon_max, grid_size + 1)\n",
    "\n",
    "lat_positions = pd.cut(df['lat'], bins=lat_bins, labels=False, include_lowest=True)\n",
    "lon_positions = pd.cut(df['lon'], bins=lon_bins, labels=False, include_lowest=True)\n",
    "\n",
    "df['location'] = lat_positions * grid_size + lon_positions\n",
    "\n",
    "df['location'] = df['location'].fillna(0).astype(int)\n",
    "df['class'] = df['class'].isin(['Good', 'Moderate']).astype(int)\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['day'] = df['time'].dt.day\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df.drop(columns=['lat', 'lon', 'source_file', 'time'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32684d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(df, year):\n",
    "    df = df[(df['year'] == year)]\n",
    "    print(f\"Dataset shape: {df.shape}\")    \n",
    "    le_location = LabelEncoder()\n",
    "    df['location_encoded'] = le_location.fit_transform(df['location'])\n",
    "    df = df.sort_values(['location_encoded', 'month', 'day', 'hour'])\n",
    "    feature_columns = [col for col in df.columns if col not in [\n",
    "        'class', 'location', 'month', 'day', 'hour', 'location_encoded'\n",
    "    ]]\n",
    "    feature_columns.append('location_encoded')\n",
    "    \n",
    "    print(f\"Feature columns: {feature_columns}\")\n",
    "    print(f\"Number of features: {len(feature_columns)}\")\n",
    "    \n",
    "    return df, feature_columns, le_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4aa64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training history for multi-step forecasting\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (per timestep)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('classical_training_history_multistep.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_example_predictions(model, X_test, y_test, num_examples=3):\n",
    "    \"\"\"Plot example predictions vs actual values\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sample_indices = np.random.choice(len(X_test), num_examples, replace=False)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5 * num_examples))\n",
    "        \n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            X_sample = torch.tensor(X_test[idx:idx+1]).to(device)\n",
    "            y_true = y_test[idx]\n",
    "            y_pred = model(X_sample).cpu().numpy()[0]\n",
    "            \n",
    "            plt.subplot(num_examples, 1, i+1)\n",
    "            hours = np.arange(1, OUTPUT_SEQUENCE_LENGTH + 1)\n",
    "            plt.plot(hours, y_true, 'bo-', label='Actual')\n",
    "            plt.plot(hours, y_pred, 'ro--', label='Predicted')\n",
    "            plt.title(f'Example {i+1}: Air Quality Prediction (Next {OUTPUT_SEQUENCE_LENGTH} Hours)')\n",
    "            plt.xlabel('Hours Ahead')\n",
    "            plt.ylabel('Air Quality Class')\n",
    "            plt.yticks([0, 1], ['Good', 'Poor'])\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('example_predictions.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af08978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SEQUENCE_LENGTH = 168\n",
    "OUTPUT_SEQUENCE_LENGTH = 72 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c17559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Builds a purely classical LSTM model for multi-step time-series forecasting.\n",
    "    Predicts the next 3 days (72 hours) of air quality classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_lstm_units=4, output_len=OUTPUT_SEQUENCE_LENGTH):\n",
    "        super(ClassicalLSTMModel, self).__init__()\n",
    "        \n",
    "        # 1. Standard LSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=n_lstm_units,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # 2. Feed-Forward Classifier for multi-step prediction\n",
    "        self.classifier = nn.Linear(n_lstm_units, output_len)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        # 1. Pass data through the classical LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # 2. Extract features from the last timestep\n",
    "        final_lstm_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # 3. Pass through classifier to get predictions for all timesteps\n",
    "        output = self.classifier(final_lstm_output)\n",
    "        \n",
    "        # 4. Apply sigmoid activation to get probabilities for each timestep\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1b32fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_pytorch(model, train_loader, val_loader, optimizer, epochs, initial_epoch=0, patience=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'val_loss': [],\n",
    "        'accuracy': [],\n",
    "        'val_accuracy': []\n",
    "    }\n",
    "\n",
    "    criterion = torch.nn.BCELoss() \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(initial_epoch, epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.numel()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch.float())\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "                preds = (outputs > 0.5).float()\n",
    "                val_correct += (preds == y_batch).sum().item()\n",
    "                val_total += y_batch.numel()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        # Append to history\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} Acc: {train_acc:.4f} Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return model, history, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_memory_efficient(df, feature_columns, \n",
    "                                    input_len=INPUT_SEQUENCE_LENGTH,\n",
    "                                    output_len=OUTPUT_SEQUENCE_LENGTH,\n",
    "                                    stride=24):\n",
    "    \"\"\"\n",
    "    Create sequences for multi-step forecasting.\n",
    "    Input: `input_len` hours of data\n",
    "    Output: `output_len` hours of predictions\n",
    "    \"\"\"\n",
    "    print(f\"Creating sequences with input length={input_len}, output length={output_len}...\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df[feature_columns])\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=feature_columns, index=df.index)\n",
    "    \n",
    "    X_sequences, y_sequences, location_indices = [], [], []\n",
    "    unique_locations = df['location_encoded'].unique()\n",
    "    \n",
    "    for i, loc in enumerate(unique_locations):\n",
    "        loc_df = df[df['location_encoded'] == loc]\n",
    "        loc_X = X_scaled.loc[loc_df.index]\n",
    "        loc_y = loc_df['class'].values\n",
    "        \n",
    "        # Calculate valid range to ensure we have enough data for output sequence\n",
    "        max_start_idx = len(loc_df) - input_len - output_len\n",
    "        \n",
    "        for j in range(0, max_start_idx, stride):\n",
    "            X_seq = loc_X.iloc[j : j + input_len].values\n",
    "            # Get the next `output_len` hours of class data\n",
    "            y_target = loc_y[j + input_len : j + input_len + output_len]\n",
    "            \n",
    "            X_sequences.append(X_seq)\n",
    "            y_sequences.append(y_target)\n",
    "            location_indices.append(loc)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed location {i+1}/{len(unique_locations)}\")\n",
    "    \n",
    "    X_sequences = np.array(X_sequences, dtype=np.float32)\n",
    "    y_sequences = np.array(y_sequences, dtype=np.float32)\n",
    "    location_indices = np.array(location_indices)\n",
    "    \n",
    "    print(f\"Total sequences: {X_sequences.shape[0]}\")\n",
    "    print(f\"Input sequence shape: {X_sequences.shape}\")\n",
    "    print(f\"Output sequence shape: {y_sequences.shape}\")\n",
    "    \n",
    "    return X_sequences, y_sequences, location_indices, scaler\n",
    " #unique_locs_subset = df_year['location_encoded'].unique()[:400]\n",
    "    #df_subset = df_year[df_year['location_encoded'].isin(unique_locs_subset)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33a197b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing year: 2021\n",
      "Dataset shape: (2035200, 58)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziyad\\AppData\\Local\\Temp\\ipykernel_32800\\839366471.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['location_encoded'] = le_location.fit_transform(df['location'])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProcessing year: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Load and prepare data for the current year\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m df_year, feature_columns, le_location = \u001b[43mload_and_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Initialize model and optimizer only once\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mload_and_prepare_data\u001b[39m\u001b[34m(df, year)\u001b[39m\n\u001b[32m      4\u001b[39m le_location = LabelEncoder()\n\u001b[32m      5\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mlocation_encoded\u001b[39m\u001b[33m'\u001b[39m] = le_location.fit_transform(df[\u001b[33m'\u001b[39m\u001b[33mlocation\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocation_encoded\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmonth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mday\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhour\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m feature_columns = [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlocation\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmonth\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mday\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhour\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlocation_encoded\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m ]]\n\u001b[32m     10\u001b[39m feature_columns.append(\u001b[33m'\u001b[39m\u001b[33mlocation_encoded\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ziyad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:7219\u001b[39m, in \u001b[36mDataFrame.sort_values\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7216\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   7217\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m7219\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   7220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   7221\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[32m   7224\u001b[39m     new_data.set_axis(\n\u001b[32m   7225\u001b[39m         \u001b[38;5;28mself\u001b[39m._get_block_manager_axis(axis), default_index(\u001b[38;5;28mlen\u001b[39m(indexer))\n\u001b[32m   7226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ziyad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    891\u001b[39m indexer = maybe_convert_indices(indexer, n, verify=verify)\n\u001b[32m    893\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ziyad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    680\u001b[39m     new_blocks = \u001b[38;5;28mself\u001b[39m._slice_take_blocks_ax0(\n\u001b[32m    681\u001b[39m         indexer,\n\u001b[32m    682\u001b[39m         fill_value=fill_value,\n\u001b[32m    683\u001b[39m         only_slice=only_slice,\n\u001b[32m    684\u001b[39m         use_na_proxy=use_na_proxy,\n\u001b[32m    685\u001b[39m     )\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    687\u001b[39m     new_blocks = [\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m         \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    695\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    696\u001b[39m     ]\n\u001b[32m    698\u001b[39m new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m    699\u001b[39m new_axes[axis] = new_axis\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ziyad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[39m, in \u001b[36mBlock.take_nd\u001b[39m\u001b[34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[39m\n\u001b[32m   1304\u001b[39m     allow_fill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1306\u001b[39m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1307\u001b[39m new_values = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[32m   1312\u001b[39m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[32m   1314\u001b[39m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[32m   1315\u001b[39m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ziyad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ziyad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    157\u001b[39m     out = np.empty(out_shape, dtype=dtype)\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[32m    165\u001b[39m     out = out.T\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "total_epochs_done = 0\n",
    "full_history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "years = [2021, 2022, 2023, 2024, 2025]\n",
    "\n",
    "for year in years:\n",
    "    print(f\"\\nProcessing year: {year}\")\n",
    "    \n",
    "    # Load and prepare data for the current year\n",
    "    df_year, feature_columns, le_location = load_and_prepare_data(df, year)\n",
    "    \n",
    "    # Initialize model and optimizer only once\n",
    "    if model is None:\n",
    "        n_lstm_units = 8\n",
    "        model = ClassicalLSTMModel(\n",
    "            n_features=len(feature_columns),\n",
    "            n_lstm_units=n_lstm_units,\n",
    "            output_len=OUTPUT_SEQUENCE_LENGTH\n",
    "        )\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Use all locations for this year\n",
    "    df_subset = df_year  # No filtering\n",
    "    \n",
    "    # Create sequences\n",
    "    X_sequences, y_sequences, location_indices, scaler = create_sequences_memory_efficient(\n",
    "        df_subset,\n",
    "        feature_columns,\n",
    "        input_len=INPUT_SEQUENCE_LENGTH,\n",
    "        output_len=OUTPUT_SEQUENCE_LENGTH,\n",
    "        stride=24\n",
    "    )\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_sequences, y_sequences, test_size=0.2, random_state=42, stratify=location_indices\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    test_dataset = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Free memory\n",
    "    del X_sequences, y_sequences, location_indices, X_train, X_test, y_train, y_test\n",
    "    gc.collect()\n",
    "    \n",
    "    # Train for this year\n",
    "    epochs_this_year = 20\n",
    "    model, history, optimizer = train_model_pytorch(\n",
    "        model, train_loader, val_loader, optimizer,\n",
    "        epochs=total_epochs_done + epochs_this_year,\n",
    "        initial_epoch=total_epochs_done,\n",
    "        patience=5\n",
    "    )\n",
    "    \n",
    "    # Plot the training history for this year\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    total_epochs_done += epochs_this_year\n",
    "    \n",
    "    # Merge history into full_history\n",
    "    for key in full_history.keys():\n",
    "        full_history[key].extend(history[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9185da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(full_history)\n",
    "\n",
    "\n",
    "X_test_plot = []\n",
    "y_test_plot = []\n",
    "for X_batch, y_batch in val_loader:\n",
    "    X_test_plot.append(X_batch)\n",
    "    y_test_plot.append(y_batch)\n",
    "X_test_plot = torch.cat(X_test_plot, dim=0).numpy()\n",
    "y_test_plot = torch.cat(y_test_plot, dim=0).numpy()\n",
    "\n",
    "plot_example_predictions(model, X_test_plot, y_test_plot, num_examples=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
